{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368dccec-1d58-4167-a50d-01c56d98c24e",
   "metadata": {},
   "source": [
    "<img src=\"../images/coefficient-pyconde.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52fb18-e4c3-4d38-86d6-cf10abeca5e8",
   "metadata": {},
   "source": [
    "# Whispered Secrets: Building An Open-Source Tool To Live Transcribe & Summarize Conversations\n",
    "## 1. Transcription\n",
    "**Questions?** contact@coefficient.ai / [@CoefficientData](https://twitter.com/CoefficientData)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dfd7f-dcc6-418c-bc6b-02c7771e07fa",
   "metadata": {},
   "source": [
    "## 0. Imports üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2dafcdd-5794-478c-88ac-d8493aadcd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import torch\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99f5e0-fea0-40bf-a43e-43701293f0f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Listen üé§Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918b349-5966-4835-8e97-461c1fb4c70b",
   "metadata": {},
   "source": [
    "<img src=\"../images/speechrecognition.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e6390-ec3e-4428-a018-f9134e7a0976",
   "metadata": {},
   "source": [
    "<img src=\"../images/sr-enginesupport.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f666b3-de51-4ebe-bfb4-c495c66500fe",
   "metadata": {},
   "source": [
    "### Configure the microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad04313-63d7-4e3c-b9e5-7251d565e90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jabra Evolve 75 SE',\n",
       " 'Jabra Evolve 75 SE',\n",
       " 'Dell USB Audio',\n",
       " 'Dell USB Audio',\n",
       " 'Jabra Evolve 75 SE',\n",
       " 'Jabra Evolve 75 SE',\n",
       " 'MacBook Pro Microphone',\n",
       " 'MacBook Pro Speakers',\n",
       " 'Microsoft Teams Audio']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr.Microphone.list_microphone_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84b8e91-13dd-4bb0-b303-201fc4ce7ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available microphone devices are: \n",
      "0: Microphone with name \"Jabra Evolve 75 SE\" found\n",
      "1: Microphone with name \"Jabra Evolve 75 SE\" found\n",
      "2: Microphone with name \"Dell USB Audio\" found\n",
      "3: Microphone with name \"Dell USB Audio\" found\n",
      "4: Microphone with name \"Jabra Evolve 75 SE\" found\n",
      "5: Microphone with name \"Jabra Evolve 75 SE\" found\n",
      "6: Microphone with name \"MacBook Pro Microphone\" found\n",
      "7: Microphone with name \"MacBook Pro Speakers\" found\n",
      "8: Microphone with name \"Microsoft Teams Audio\" found\n"
     ]
    }
   ],
   "source": [
    "print(\"Available microphone devices are: \")\n",
    "for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "    print(f'{index}: Microphone with name \"{name}\" found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746e9b26-220f-43b2-b592-65528c1b3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_index = 0\n",
    "#int(input(\"Please enter the index of the microphone you want to use: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d0a3a2-691f-473e-a5aa-9012f3c4cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = sr.Microphone(sample_rate=16000, device_index=mic_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26fa69-d88b-4b24-9b52-4dc07c31cd63",
   "metadata": {},
   "source": [
    "### Listen & transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128e7758-4a87-44bb-b195-87ca70bcc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78fe50eb-44d9-4c36-9803-b2fc03ae5aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    audio = recorder.listen(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f909df-e84c-46c8-8f17-dc1df65bb25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper thinks you said: 'Hello?'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\n",
    "        f\"Whisper thinks you said: '{recorder.recognize_whisper(audio, language=\"english\").strip()}'\",\n",
    "    )\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Whisper could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(f\"Could not request results from Whisper; {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449482c-7109-4b00-ae20-af3416723825",
   "metadata": {},
   "source": [
    "### Live transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83793357-a1b7-478e-8abe-42ad6b863ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = whisper.load_model(\"tiny.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a64a931e-4d00-4c47-b448-5a00935631b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpeechRecognizer will detect when speech ends.\n",
    "recorder = sr.Recognizer()\n",
    "\n",
    "# Energy level for mic to detect.\n",
    "recorder.energy_threshold = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38dc3fe0-ed10-4d3e-88ad-03a4c0b99ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic energy compensation lowers the energy threshold dramatically to\n",
    "# a point where the SpeechRecognizer never stops recording.\n",
    "recorder.dynamic_energy_threshold = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9881011-f6f7-4188-97ae-d96c5ed18630",
   "metadata": {},
   "outputs": [],
   "source": [
    "with source:\n",
    "    recorder.adjust_for_ambient_noise(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cfdce00-f643-4f52-9397-23fecd60c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread safe Queue for passing data from the threaded recording callback.\n",
    "data_queue = Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da6a0aa3-34cb-4eb6-8372-ef97034a674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_callback(_, audio: sr.AudioData) -> None:\n",
    "    \"\"\"\n",
    "    Threaded callback function to receive audio data when recordings finish.\n",
    "\n",
    "    audio: An AudioData containing the recorded bytes.\n",
    "    \"\"\"\n",
    "    data_queue.put(audio.get_raw_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7678089-2945-4029-81be-df25ee108959",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = [\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7c956-5a91-4e59-8617-a2b579686272",
   "metadata": {},
   "source": [
    "#### üëá **START TALKING!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4db59b9a-03a5-456e-936d-4a65863100b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'tiny.en' loaded & listening...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How real time the recording is in seconds.\n",
    "record_timeout = 2.0\n",
    "\n",
    "# Create a background thread that will pass us raw audio bytes.\n",
    "# We could do this manually but SpeechRecognizer provides a nice helper.\n",
    "recorder.listen_in_background(\n",
    "    source,\n",
    "    record_callback,\n",
    "    phrase_time_limit=record_timeout,\n",
    ")\n",
    "\n",
    "print(\"Model 'tiny.en' loaded & listening...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7de167f3-5e16-4482-93f4-52907b9fabb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_queue.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdb913d5-b433-422a-b154-6feb910e913b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<queue.Queue at 0x134be5ac0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b991d5de-1302-49ec-b073-3b9d9a238f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine audio data from queue\n",
    "audio_data = b\"\".join(list(data_queue.queue))\n",
    "data_queue.queue.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c920496f-3fb5-45d1-95cb-e7e95cfe626a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68f1ea68-5b51-48da-a939-4ad74bfb7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea7c0d9c-5483-4a13-82cd-13b261996497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play the audio\n",
    "sample_rate = 44100\n",
    "Audio(audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fccbc8c7-948b-47e7-8ba8-af6b8b9a853b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert in-ram buffer to something the model can use directly without needing a\n",
    "# temp file. Convert data from 16 bit wide integers to floating point with a width\n",
    "# of 32 bits. Clamp the audio stream frequency to a PCM wavelength compatible\n",
    "# default of 32768hz max.\n",
    "audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "audio_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7051d48-9763-4bab-9f9c-ef8a10026192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Audio(audio_np.tobytes(), rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd6bda4c-ba84-40c4-a70e-fe4cded8a810",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '', 'segments': [], 'language': 'en'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the transcription.\n",
    "result = audio_model.transcribe(audio_np, fp16=torch.cuda.is_available())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50a355-3e1f-4ad5-863c-2cd979d590d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, I'm roasting. Hello, I'm roasting.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = result[\"text\"].strip()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77990f-173b-40ea-924c-dd98e78d50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aea266-712f-4f6a-84fe-58779ab480f5",
   "metadata": {},
   "source": [
    "# 2. Live transcription demo - run `python -m demo.transcribe` from repo root üîä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e8a1d-469c-454a-aca0-56fab622dfae",
   "metadata": {},
   "source": [
    "<img src=\"../images/transcribe.gif\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b2486-8e0b-4a4e-869f-08a2c9ecdd51",
   "metadata": {},
   "source": [
    "### Change #1: typer CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e0e2a-f18b-4054-b3a7-44650ae7dd53",
   "metadata": {},
   "source": [
    "<img src=\"../images/typer.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276ba8e-fdee-4db1-8f29-d4a7bd4913ea",
   "metadata": {},
   "source": [
    "<img src=\"../images/typer2.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f443ab-dcfd-4316-ae0d-15dead361d3c",
   "metadata": {},
   "source": [
    "### Change #2: Load tiny, small, medium models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c2ac9-0e69-4591-8085-80dcabeed948",
   "metadata": {},
   "source": [
    "<img src=\"../images/load-models.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcafa69-b0b5-4082-84bb-dc492497bb54",
   "metadata": {},
   "source": [
    "### Change #3: Infinite loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59edf77-8e35-4bb4-88f9-d43829f4452b",
   "metadata": {},
   "source": [
    "<img src=\"../images/loop.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb8c44-fa97-4cc4-a4f5-8bec94cbeced",
   "metadata": {},
   "source": [
    "### Change #4: Phrase detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f804bdf-073d-49e3-bb14-baa4f9c6b8bb",
   "metadata": {},
   "source": [
    "<img src=\"../images/phrase1.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a3a50-53bb-4efe-86e3-1fc5fbdd4bb1",
   "metadata": {},
   "source": [
    "<img src=\"../images/phrase2.png\" width=800>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
